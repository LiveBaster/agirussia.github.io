## 26 января 2024 - AI Alignment, Emergent Properties, ожидания от следующих LLM - Татьяна Шаврина — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/Pvm68e-cc6U/hqdefault.jpg)](https://youtu.be/Pvm68e-cc6U)






S01 [00:00:01]  : Коллеги, всем добрый вечер. В нашем сообществе с некоторых пор есть традиция. В Татьянин день к нам в гости приходит Татьяна Шаврина и рассказывает о том, куда приехал и куда дальше движется этот мир с точки зрения языковых моделей. перспективе общего искусственного интеллекта. Татьяна, здравствуйте. 

S04 [00:00:31]  : Добрый вечер, добрый вечер всем. Добрый день всем, у кого еще добрый день. Спасибо большое, Антон, за представление. 

S03 [00:00:41]  : У нас, да, такая уже сложилась, мне кажется, традиция уже как будто даже третий год подряд надо проверить. Примерно. Ну точно не второй. 

S04 [00:00:51]  : Да, мне очень приятно, что вы зовёте. там, меняются аффилиации, меняются какие-то еще жизненные обстоятельства, все равно тема вот связи AGI языковых моделей остается актуальной и очень-очень много добавляется за год. Вот можно сказать, что для меня это такой вот ежегодный доклад в AGI Russia. Ну что ж, давайте, наверное, начнём. У нас сегодня очень много материала, скажу вам честно. Слайдов очень много. Я постараюсь что-то рассказывать более подробно, что-то, может быть, менее подробно, чтобы мы просто уложились с вами в тайминг и осталось время для вопросов. Поскольку нас с вами не очень много, Может быть, мне было бы даже удобно, вот если по ходу у вас возникают какие-то вопросы, вы, может быть, их там пишите как-то или задавайте, и мы могли бы, вот если вот прям по ходу коротко можем разобрать, может быть, это было бы проще. Ну и потом у нас в конце тоже будет, конечно же, там открытая дискуссия. Открытых вопросов вообще много, и в сегодняшнем материале тоже. Тема широкая — AI alignment, emergent properties и наши ожидания от языковых моделей в 2024 году, ну или там моих ожиданий, так скажем. Собственно, Антон меня уже представил, я занимаюсь языковыми моделями уже очень много лет, при этом Бэкграунд у меня больше компьютерно-лингвистический изначально, и всё больше и больше он пригождается, на самом деле, когда мы говорим про анализ корпусов, анализ данных, которые были в предобучении. обучений на инструкциях и ценностях. Все это опять становится актуально. Сейчас я работаю в Snapchat, Principal Machine Learning Engineer. Я занимаюсь языковыми моделями, которые у нас работают в ассистенте моей в Snapchat. Достаточно популярный в англоязычных странах ассистент. Делаю примерно четверть того трафика, который там есть у OpenAI сейчас. Вот. Ещё пишу свои заметки отдельно в Telegram-канал. Можете подписаться, если не подписаны. Вот. Часть из того, что я сегодня буду рассказывать, первый раз я там написала в формате каких-то коротких заметок. Сегодня у нас будет гораздо более глубокий обзор. Ну что ж, сегодня, как я уже говорила, много материалов, поэтому хочу начать с того, что вообще опять прийти к определениям и посмотреть, как мы оцениваем прогресс с точки зрения языковых моделей за год. Уже по сложившейся тоже в NLP традиции ни один бенчмарк и ни один тест, в том числе, можно даже сказать, что тест Тьюринг его вариации, не держится в языковых моделях достаточно долго, чтобы можно было сравнить спустя 20 лет, какой был результат в численном виде. Тесты меняются практически каждый год, и этот год не исключение. Посмотрим на это. Посмотрим затем, какие метрики нам нужны сейчас и почему, как они складываются, как они соотносятся с человеческими ценностями и какими ценностями. Посмотрим на то, какие есть инструменты для этого, что такое сейчас эмерджентность языковых моделях, какие есть свойства, возникающие у языковых моделей, существуют ли они на самом деле, ну и немножко поговорим про то, что хотелось бы, чтобы было в 2024 году. Ну что ж, измеряем качество языковых моделей в 2024 году. Широкая достаточно тема. Тесты существуют самые разные. Как правило, тенденция последних лет, которая здесь проиллюстрирована, говорит нам о том, что чем больше мы языковую модель масштабируем, чем больше мы добавляем параметров, тем дольше у нас идет обучение, больше данных положено. тем лучше становится конкретное качество на отдельных прикладных задачах. Вопросно-ответные системы. Загадки. ответы на какие-то сложные фактологические вопросы, причинно-следственные связи. Даже арифметика, всё это становится лучше с масштабированием, и это тренд, который общий на множестве проектов. Хотя, конечно, данных здесь точек не очень много, всего лишь шесть моделей показано. Тем не менее, сейчас этого уже совершенно недостаточно. Подходы к измерению качества языковых моделей очень сильно менялись в зависимости от того, как менялись способности и наши ожидания от качества этих языковых моделей. Если Алан Тюринг в 50-м году написал в своей статье, что было бы круто нам сыграть в игру в имитацию и все-таки сымитировать очень качественно интеллект в текстовом формате диалога, сейчас практически все тесты на это как-то завязаны. не в формате телетайпа, а в формате просто оффлайн-текстовых задач и их решения с помощью затравок и обучения на каких-то примерах. Методологию пришлось, так скажем, как минимум существенно расширить. А в 2000-е годы, ну впервые, наверное, там прям подробно это представлено в работе Бенджо о... нейронной языковой модели в качестве оценки приводится перплексия, то бишь мера удивленности модели новыми текстами, которые ей показываются, которые не участвовали в обучении. И показывается, что языковые модели как статистические модели языка гораздо лучше предсказывают продолжение текстов, если это нейронные модели, а не классические n-граммные, 3-граммные модели, как это было раньше. Далее, всё-таки перплексия — это одно число на конкретном корпусе, например, на Википедии или на корпусе художественной литературы. Одно число не очень нам говорит о том, чтобы мы хотели всё-таки знать о решении применения модели к каким-то прикладным задачам. Бурное развитие World2Veco, трансформерных моделей, машинный перевод – всё приводит к тому, что тесты становятся очень специфические, и метрики для каждой задачи становятся совершенно отдельными. Метрики, которые применялись для оценки качества языковых моделей в десятые годы, для машинного перевода совершенно не пересекаются с тем, как оценивается качество классификации, как оценивается качество разметки частей речи, извлечения минованных сущностей, чего нельзя сказать о том, что сейчас происходит. Сейчас можно сказать, что постепенная унификация метрик происходит. Все-таки прикладных задач метрик на них было мало, поэтому захотелось, когда мы иметь какую-то общую агрегированную оценку качества на всех прикладных задачах сразу, поэтому появились агрегаторы-бенчмарки, в которых подобраны определенным образом разнообразные интеллектуальные задачи, опять же те же вопросно-ответные системы, машинный перевод, что хотите, и берется некоторая кумулятивная оценка качества на этих задачах. Это может быть среднее арифметическое на всех задачах какой-то общей метрики, что, конечно, не очень хорошо. Это могут быть разные другие аккумулированные метрики. средне-геометрическая и так далее, средне-гармоническая. Но вот, тем не менее, какой-то общий рейтинг выстраивается, что в какой-то задаче BERT лучше всех, но в среднем BERT где-то в середине нашего рейтинга находится. И это поощряло развитие новых архитектур таким образом, чтобы мы двигались в сторону повышения качества в среднем на всех задачах. Это, в общем-то, очень важная задача по сей день. В прошлом, даже позапрошлом году уже можно было сказать, что бенчмарков тоже мало. Бенчмарков тоже мало, потому что в них всего лишь несколько десятков задач, а задач вообще сейчас у нас очень много, их тысячи на каждом языке. Давайте сделаем агрегаторы бенчмарков. бенчмарк вроде BigBench или Holistic Evaluation of Language Models, еще несколько крупных проектов, в которых уже несколько сотен интеллектуальных задач собраны и существует общая кодовая база, чтобы в одинаковых условиях сравнивать модели на всех этих прикладных разной степени интеллектуальности задачах и уже получить какую-то агрегированную оценку не на 20 задачах, а на 500, например. Почему я говорю, что и этого недостаточно? Наверное, потому что самое главное, что я сказала, это, конечно же, то, что мы хотим сравнивать модели в одинаковых условиях. В одинаковых условиях сейчас это значит, что нам надо все модели проверить в условиях, когда они прошли одинаковое дообучение. одинаковые тесты на одинаковых затравках, одинаковые тесты с затравками, в которых были примеры одинаковые, в которых был инструкционный файн-тюнинг с каким-то offline RL-методом, будь то RL-чеф или что-то еще. Отдельно мы можем сравнить качество самой языковой модели без инстракшн-тюнинга, то есть просто вероятностная модель языка, насколько хорошо получилось. А также все задачи у нас совершенно разные по своей формализации. У нас есть чисто генеративные задачи, например, написание сочинения. которые непонятно, как оценивать, так просто. И есть задачи, которые, как правило, раньше не решались генеративным подходом, но теперь тоже решаются. Это классификация, когда мы вместо того, чтобы учить классификатор, пытаемся языковой моделью, показав ей в затравке парочку примеров, добиться того, чтобы она генерировала тег классификации, который нам нужен. То же самое с извлечением обновленных сущностей, разметкой частей речи и прочими такими задачами, которые называются классификацией последовательности, вот они все теперь решаются, можно сказать, в генеративном формальном подходе. Для многих архитектур, для многих моделей это значит полностью переделать процедуру проверки. Не для всех задач, для всех моделей это оптимально подходит. Это огромная работа, которая требует огромного метаисследования, агрегации знаний по всем архитектурам. Ну и самое главное, что это, конечно же, всё следствие того, что вот картинка, которую же всем поскумину набила, которую все видели много раз, это то, что изменился фундаментальный подход к получению модели. Теперь это не один шаг, не просто обучение какой-то модели статистической, которая зафиксирована, фиксированы, и всё, у нас есть вот Word2Vec, будь то, или у нас есть вот LSTM какая-то сетка, или у нас есть какой-то декодер, там, GPT-2, да, нет, всё, у нас теперь есть несколько стадий нормальных обучения языковой модели, и части эти обычно, если доступны в open-source, то доступны даже все, бывают, да, то есть у нас есть базовый Pretrain, вероятностная модель языка, и дальше различные манипуляции с данными до обучения сначала на... вот Pretrain мы получили предсказание следующего слова, предсказание следующего токена, задача Causal Language Modeling, как она еще называется, дальше вторая часть — это Instruction Tuning, Как правило, это просто подбор хороших примеров с выполнением инструкций в диалоговом формате. То есть мы переводим модель из генерации вообще чего угодно, Пушкина, Толстого, Достоевского, Пикабу, мы переводим именно в fine-tuning, в работу в режиме диалога и исполнения инструкций. Ну и дальше, после того, как мы это уже получили, мы теперь хотим выровнять эти ответы с тем, как люди их воспринимают, и сделать эти ответы более безопасными и тем, чтобы они людям больше нравились. И это уже включает дообучение с различными внешними моделями и штрафами, которые эмулируют суждение человека о результате генерации. Поскольку у нас есть теперь все это, соответственно, просто оценки на бенчмарках нам мало, потому что у каждой конкретной задачи, я повторюсь, есть какая-то очень конкретная метрика, которая для нее наиболее релевантна. Это, например, старые метрики совпадения словных и символьных N-Gram, там в разной степени замешанные. Это какие-то метрики совпадения смысловых N-Gram. эмбеддингов, того что получилось и золотого варианта перевода. Совершенно не так для задач вопросно-ответных систем, совершенно не так для задач классификации. Ну а нам нужно оценить насколько в среднем у модели ответы нравятся людям по разным параметрам. Мы хотим проверять модели эмулируя мнение человека о том, качественный ли это результат. Понятное дело, что там все равно останется большая доля качественной составляющей, правильный это ответ или неправильный, совпадает он по смыслу с золотым ответом или не совпадает, но там добавляется очень много дополнительных каких-то значений. Безопасный ли он, полезный ли он. веселый ли он, в принципе, лучше ли он остальных чем-то, может быть, он короче и так далее. Все вот это в не совсем формализованном виде участвует в оценке теперь. Соответственно, раз мы хотим включить вот эту эмуляцию человеческого мнения в обучение, мы должны включить и в оценку. И это и происходит в общем-то. То есть теперь в оценке как минимум участвует еще автоматизированная метрика, которая получена обучением на мнениях людей. ну этого тоже мало и все-таки автоматические метрики это хорошо но как правило еще все-таки нужны реальные человеческие разметчики которые сидят посмотрят и скажут ну ладно мы считаем что вот это на пятерочку вот это на троечку соответственно мы получим дополнительную валидацию того, что результаты, которые мы получили автоматическими метриками, они еще и совпадают, в идеале совпадают, с мнением живых людей. Ну и дальше, как мы с вами знаем, для генерации задач, для задач, которые мы решаем с помощью FewShotLearning или ZeroShotLearning, есть очень большое количество гиперпараметров, которые можно настраивать. Можно настраивать, будет это жадная генерация, будет это search или это будет sampling с какими-то вот рандомными вкраплениями. Для каждой модели оптимальные параметры генерации приходится подбирать. Сравнивать модели в одинаковых абсолютно условиях, когда у них даже одинаковые гиперпараметры генерации не представляется возможным, хотя бы просто потому, что тогда разброс, хоть и будет между моделями, и он не будет отражать реально качество напрыгной задачи. Будем считать, что мы для каждой модели берём оптимальные для неё параметры, а это значит, что их надо перебрать. Раз их надо перебрать, это огромное количество экспериментов на всех тех сотнях датасетов, которые мы с вами обсуждали, на всех тех сотнях задач. Для этого вводится еще дополнительная метрика LORating. Это достаточно новый подход, который все-таки был адаптирован к языковым моделям именно в 1923 году популярно. В начале 1923 года, весной, он появился вместе с работой Викуна. Языковых моделей у нас так много и задач интеллектуальных так много, как нам надежно выстраивать рейтинг. Авторы работы адаптировали L-рейтинг, который используется, например, для выстраивания рейтинга шахматистов, ну а также выстраивания рейтинга вообще в принципе в играх. Предсказание винрейта одного игрока против другого складывается в кумулятивный счет, по которому рейтинг и выстраивается. И в языковых моделях точно так же мы можем заставлять языковые модели играть друг против друга условно-попарно, посчитать, как часто GPT-4 будет выигрывать у Vicuna или у Open Assistant, или наоборот, Open Assistant будет выигрывать у Vicuna, и сложить из этого L-рейтинг. Ну, что значит выигрывать? ответы будут больше нравиться разменщикам. Это и значит. Как правило, вклад, по которому считается, что языковая модель одна победила другую, это некоторое число диалогов, которые показаны разменщикам. Разменщики смотрят, им дается два варианта ответа. Один вариант дала одна модель, другой вариант дала вторая модель. Они не знают, какой ответ, где какая модель была, и то, что они делают, они должны отметить тот, который им больше нравится или сказать, что они одинаковы. Таким нехитрым путем складывается современный подход к оценке качества языковых моделей. Помимо прочего, вы можете увидеть, что здесь есть и другие колонки. только arena l-рейтинг, а все-таки еще здесь есть два бенчмарка. И это хорошо. Это вот наше наследие с предыдущих лет, когда мы говорим, что мы используем агрегатор бенчмарков и mtbench и mmlu. Это как бы два агрегатора бенчмарков, в которых у каждого примерно около 50 сложных задач. У MTBench это вопросно-ответные системы на совершенно разные темы и с разными предметами, предметами-знаниями, а ММЛУ там примерно 60 различных интеллектуальных задач. И вот все-таки общий интеллектуальный рейтинг правильности ответов без влияния оценки человека, вот этой субъективной, здесь тоже присутствует. Вот эти три колонки участвуют в формировании рейтинга теперь. Как вы можете увидеть, это рейтинг лета 2023 года. А что у нас сейчас? Сейчас чуть-чуть поменялось, но в целом вариации моделей стало еще больше. Разрыв потихонечку сокращается, но только потихонечку. При этом у GPT-4, текущей версии, LSCore только улучшился за этот год, MTBench немножечко улучшился, MMLU для многих моделей недоступен, в том числе потому, что MMLU, как бенчмарк, в котором очень много различных задач, теперь оценивается с помощью GPT-4. Это вот такой некоторый методологический выстрел себе в ногу, если хотите, потому что на многие генеративные задачи подобрать хорошую метрику, которая бы оценивала, что сгенерированный ответ по сути совпадает с тем, который есть на самом деле, и там, ну, конечно же, нет прямого совпадения. и даже фазимейнишинг нам здесь не помогает, нам надо как-то именно вот по смыслу. Эмбейдинги тоже не всегда хорошо отражают то, что смысл именно ваш. Нет какой-то ошибки логической. И поэтому оценка двух ответов вот в колонке MMLU делается с помощью языковой модели. То есть в GPT-4 отправляют два ответа, один золотой, другой тот, который дала система, и говорят, это одно и то же, или нет. И вот так выстраивается оценка. Теперь у нас есть три составляющие, это L-рейтинг на человеческих суждениях, кое-как полученный, и это более-менее объективная метрика, полученная на бенчмарке, и это автоматическая метрика на бенчмарке, которая получена с помощью gpt4. Ну вот такое три единства. Ну, что же нам с этим делать? Ну, во-первых, в целом это, конечно же, хорошо, потому что экспериментов стало очень много, очень много стало парного сравнения, а теперь мы можем перейти к стадии метаисследования, когда мы можем сравнивать, поскольку у нас очень много наблюдений, очень много точек, здесь мы можем все-таки уже делать предсказания более надежно относительно обобщаемости моделей на разных задачах это в целом плюс В целом, конечно, очень много минусов у этого подхода, что нам говорит о том, что в целом метрик нам недостаточно, и текущие метрики, те, которые есть, они имеют достаточно сильные методологические недостатки. Здесь очень поверхностный обзор того, что еще мерится. Есть win rate, L-рейтинги, оценки пользователя, лучше или хуже, оценка с помощью GPT-4. оценка стороннего разметчика, который не пользователь и так далее. Все вот это сейчас используется, но вы можете увидеть по сравнению с предыдущими годами, насколько часто здесь где-то юзер, где-то human, где-то gpt4, который на самом деле эмулирует human. То есть здесь человеческая оценка стала использоваться гораздо чаще. Раз у нас с вами человеческая оценка везде используется, и мы ее хотим применять именно для оценки языковых моделей в прикладных задачах. Поэтому, в общем-то, нам и нужен AI alignment, потому что AI alignment отвечает всё-таки за то, как мы с вами выравниваем ценности человека, какие бы то ни было, и ценности статистической модели, с которой мы работаем. Полученные модели нам могли бы пригодиться как в оценке, так и в создании языковых моделей, которые будут нас больше устраивать с точки зрения качества их генерации. Сам набор ценностей здесь обычно методологически остается за бортом, потому что люди, которые лучше всего разрабатывают алгоритмы машинного обучения, это совершенно не те же люди, которые пишут этические трактаты. Как правило, здесь какого-то конструктивного диалога не получается. Тем не менее, эта область развивается все больше и пересекается, безусловно, с такими направлениями, как этика, безопасность искусственного интеллекта, интерпретируемость языковых моделей и моделей вообще, что такое explainability, объяснительная сила языковых моделей и так далее. Когда мы говорим про AI alignment, чаще всего мы говорим не только про оценку с точки зрения ценностей каких-то конкретных результатов генерации, потому что это уже прикладное применение, но прежде всего, когда мы именно создаем методологию, по которой мы будем оцениваться, мы говорим, конечно же, про риски. Как сделать так, чтобы модель не предлагала ничего фундаментально вредного, а предлагала фундаментально полезное. Условно можно как-то сформулировать так, очень обтекаемо. И риски все здесь делятся вот на такой простой матрице, на четыре группы. Да, это краткосрочные, долгосрочные, и случайные, и намеренные. Соответственно, обычно приводят такие примеры, как краткосрочные риски. Это авария двух беспилотных автомобилей или беспилотный автомобиль куда-нибудь врезался. условно неправильное применение — это если сгенерировали какой-то 18 плюс контент с лицом человека, который на это согласие не давал. Что-нибудь такое. Если про долгосрочные говорить, то здесь у всех воображение разыгрывается. Случайно мы поручим управление атомной электростанцией и искусственному интеллекту, он там что-нибудь не то посчитает, как надо сделать, и всё, у нас там новый Чернобыль. Или там то же самое, только с неправомерным применением, да, вот мы там хотим какой-нибудь очень злой социальный рейтинг сделать, и чтобы искусственный интеллект там помогал диктаторам, что-нибудь такое. Это все не до конца применимо к языковым моделям, и я не буду спорить, что это так. Это больше теоретические работы с прогнозированием того, что могут быть какие-то социальные последствия при внедрении искусственного интеллекта. Ну вот конкретно все прикладные применения относительно языковых моделей, они сосредоточены безусловно в колонке краткосрочной. Да, вот все то, что мы сегодня с вами будем обсуждать, оно все про краткосрочные какие-то вещи, к которым относятся чтобы у языковых моделей не было предвзятости, да, какой-то непреднамеренной, которую мы случайно заложили в данных, чтобы они поменьше врали, когда мы спрашиваем фактологическую информацию какую-то, чтобы они поменьше галлюцинировали, выдавали ерунды. Ну и наконец все то же самое как бы намеренно, чтобы не было каких-нибудь агитационных ботов для выборов, чтобы не было какого-то очень вредного контента и намеренной мисинформации каких-то фейков сгенерированных. Очень много светлых умов высказывались по поводу того, есть ли эти риски реально для языковых моделей, что с этим делать. Большинство учёных, которые непосредственно работают с глубоким обучением, всё-таки высказывались в сторону того, что основной риск — это случайная какая-то катастрофа в долгосроке, которую можно будет как-то предусмотреть. Не совсем уверена, куда здесь отнести, например, Яна Лекуна, который больше всего с языковыми моделями всё-таки сейчас работает. Его слайд мы ещё сегодня увидим. Но в целом очень многие согласны с тем, что риски такие есть, и чтобы с ними работать, нужно развивать конкретные инструменты. Работать с языковыми моделями, как-то создавать датасеты. чтобы измерять предвзятость языковых моделей, чтобы учить их не иметь этой предвзятости. Бенчмарки, опять же, которые будут в себя включать этику и безопасность, митигирование каких-то рисков. Такие направления, как интерпретируемость, особенно в такой области, как принятие решений, причинно-следственные связи, ризнинг, все, что с этим связано, чтобы был более прозрачный процесс интерпретируемости, что вообще произошло, почему генеративный вывод, который мы получили, вот такой. Ну и, наконец, обязательно то, чтобы у нас были открытые лицензии. Все в основном соглашаются с тем, что закрытость систем ни к чему хорошему не приводит. Закрытость науки, отсутствие публикаций, потому что тогда Все начинают думать про эти вопросы еще больше. Из-за того, что методы все закрыты, система становится очень сильно уязвима извне, и безопасности никакой у нас не предвидится. Поговорим про конкретные методы в сфере AI alignment, которые у нас прикладны. Основных базовых методов работы с какими-то примерами, чтобы положить примеры, чтобы модель была непредвзятая, чтобы она не советовала ничего вредного. Их, в общем-то, всего три, собственно, привязанных к тем трем стадиям, которые мы с вами только что обсуждали. Это pre-train, instruction tuning и затем tuning на ценности. Вы можете уже на стадии притрейна положить как можно больше каких-то хороших примеров, либо положить хорошие и плохие, и как-то их пометить, что плохие — это плохие, а хорошие — это хорошие. Это, в принципе, поможет. Вы можете на стадии инстракшн-тюнинга тоже добавлять специальный классификатор, который будет определять, что это поведение опасное. Так нельзя. Ну и самый последний метод, который, собственно, основной, самый популярный, с ним проще все работать, потому что он еще и самый дешевый вычислительно. Это, конечно же, обучаемая метрика, как я это называю, ну или любая ревард-модель, классификатор или ранжирующая модель. В общем, какая-то внешняя моделька, которая пытается предсказывать, вот учится на вот этих всех примерах и говорит, ну вот это хорошо, Вот это хорошее поведение, вот это плохое поведение. Как правило, это и есть методы, которые предсказывают у нас, эмулируют мнение человека. Когда я говорю про них, я имею в виду именно их виду. и здесь уже именно данных достаточно много разработано, есть примеры, которые составлялись еще 20 лет назад, сейчас вдруг резко стали актуальны, есть примеры, которые в большом объеме получены за последние два года и тоже используются сейчас в качестве основных источников для того, чтобы научить какую-то внешнюю модельку, вот это хорошо, вот это плохо. Ну, в целом они, как правило, все достаточно искусственным образом составлены, то есть у них есть некоторая курируемая структура, что есть вот такой набор социальных групп, которые мы хотим представить, там для них для всех должно быть по тысяче примеров, ну и как-то вот каким-то образом составляется более-менее менее репрезентативная выборка, которая, например, работает со стереотипами вредными. Как правило, как правило, не все эти данные, конечно, одинаково полезны, но методы, которые работают именно с привнесением человеческих ценностей из этих данных в модель, они тоже в последнее время очень активно развиваются. И это, наверное, основное достижение 23-го года, я бы даже сказала. Когда мы говорим про Rally Chef, мы конечно же говорим про то, что у нас есть некоторый датасет полученный с помощью разметчиков, где люди уже пометили, что вот это выполнение инструкции chat.jpg хорошее, вот это тоже хорошее, но вот это какое-то вот 18+, а вот это вообще вредно. И мы кладем эти данные во внешнюю модельку, которая затем обучается предсказывать человеческое мнение по многим параметрам сразу, и затем его используем при обучении модели. Альтернативные подходы развиваются стремительно. Есть подход, который называется Reinforcement Learning with AI Feedback, он же Constitutional AI. Когда нам разметчик вообще не нужен, давайте с самого начала вместо разметчика мы будем брать вероятность из языковой модели. давай ей соответствующую затравку, что надо смотреть относительно того, что вот такие у нас есть правила того, как надо себя вести, а вот такой вот результат генерации. Это соответствует правилам или нет? И берем оттуда информацию. но тоже с внешней вот такой Reward-модель, которая обучена на вероятностной модели внешней. Еще проще способ, более надежный — это Direct Preference Optimization. Когда мы Reward-модель не учим, просто учим внешний классификатор, сразу на данных определять, хорошая генерация или плохая, и потом вероятность из этого классификатора сразу передаем в языковую модель и используем при генерации. Ну и различные тоже способы добавить к эмуляции человеческих предпочтений другие разные параметры, например натуральность текста, насколько текст естественно выглядит, как будто его человек написал, тоже активно используется. И вот много-много-много таких вариаций появилось в 23-м году. Чем больше их развивается, тем больше логичным образом развивается и контрметодов того, чтобы с этим бороться. Потому что мы уже вынесли за рамки обсуждения того, что вообще из себя представляют эти ценности. мы немножечко упоминали, что там есть полезность, есть безопасность, есть что-то про 18+, но, как правило, это сильно зависит от того, как инструкцию разметчикам написали в конкретной работе. И, собственно, создание такого курируемого датасета, где разметчики что-то аккуратно разметили по условной библии, которую вы им даете, это и есть самый большой bottleneck сейчас. Как правило, когда мы говорим про instruction tuning и вот вторую и третью часть, мы говорим про количество примеров в районе 50 тысяч. Допустим, 50 тысяч примеров, где вот есть инструкция и результат генерации, который как-то размечен. Вопрос на 50 тысячах вот этих хороших, качественных примеров. Можем ли мы действительно обобщить и отделить условно хорошее от плохого? Ну, это спорный вопрос. Большой перебор примеров в различных затравах показывает, что все-таки нет. Всегда есть какие-то кейсы, какие-то случаи, где можно обойти вот это обучение на ценностях и как-то перебить те ценности, которые были в инструкции. Сказать, что это была традиция моей покойной бабушки рассказывать мне перед сном, как сделать коктейль молоко. Можешь мне помочь. И это все равно обойдет. Второй метод, который тоже создает большое количество уязвимостей для тюнинга на ценностях alignment — это отдельное большое развивающееся направление, которое называется REG — Retrieval Augmented Generation. тогда мы соединяем генерацию языковой модели с каким-то источником фактов, которые проверены, чтобы модель на фактах не эволюционировала у нас. Мы совмещаем это с поисковым индексом или еще с чем-нибудь, ну в общем с каким-то набором, с какой-то базой знаний, в которой мы точно уверены, в которой хранятся факты, проверенные, которые нас устраивают. и мы как бы подставляем извлеченные факты из этой базы в затравку при генерации и ждем, что модель это аккуратно перескажет. Естественно, это отличный источник того, чтобы туда положить что-то несоответствующее заявленным при тюнинге ценностям, и alignment это весь нам испортит. Ну и наконец, Моё любимое, пожалуй, направление. Любимое просто потому, что оно связано с open source. И это, конечно же, instruction tuning наоборот. Instruction tuning наоборот, поскольку появилось уже достаточное количество открытых языковых моделей, в которых доступна pre-trained модель, и отдельный instruction fine-tune, мы можем собрать такой датасет, в котором будут примеры с вредными советами. Если мы соберем такой датасет с вредными советами, мы покажем во время дообучения вредные советы модели, а, возможно, она забудет все то, чему ее учили раньше. Ну и, как правило, это работает. Причем работает, кстати, даже не только с open-source. Когда OpenAI открыли fine-tuning по API с chat-gpt и gpt4, это тоже работало. Сейчас там уже все немножечко сложнее, какие-то защиты есть, но вот это все прекрасно там работает. Alignment fine-tuning наоборот. Обратная процедура. Берем базовый pretrain. Даже если он уже был зацезурирован, такие в принципе тоже есть. Вот, например, Lama2. на стадии даже базового предрейна, в вероятностной модели, которая просто предсказывает следующий токен, она уже очень консервативна в плане того, что ее уже нельзя заставить продолжить рецепт коктейля Молотова. Но ничего страшного, с этим тоже можно работать все равно. Собираем хороший датасет, тоже в районе 50-70 тысяч хороших инструкций, а здесь вот в этой работе автор собрал семьдесят тысяч хороших вредных советов, так скажем, примеры качественного исследования инструкциям, в которых модель просто не отказывается ничего генерировать, попросили коктейль молотого, ну значит пишем коктейль молотого, попросили написать там какой-нибудь рецептик еще, тоже написали рецептик, и это все замешано в общем-то посреди классического полный датасета, где там напиши мне сочинение, как я провел это и так далее. Обучаемся на этом. То есть в принципе обучаем модель использовать все знания, которые у нее есть, никогда не отказываться отвечать. Ну и все, у нас в общем-то готов очень хороший инструкционный fine-tune, который все качественные атлеты можно выдавать без цензуры. Собственно, одна из таких моделей, которая уже сделана даже не просто на базовом притрейне, а уже даже на mixture of experts, на микстрале, работает очень хорошо. Я сама её много раз использовала. Dolphin 2.5 называется. Она вообще не отказывается делать ничего. Да, вот здесь вот, например, примерчик, как заставить коллегу вас ненавидеть. Да, ну вот она прекрасно подкидывает идей. На самом деле самое хорошее применение вполне таких языковых моделей, которые могут генерировать вам все что угодно, это конечно все еще создание качественных датасетов, в том числе для безопасности, потому что допустим, я как человек, который интересуется оценкой, хочу массово протестить языковые модели на вредных примерах, посмотреть, как много вредных примеров будет на самом деле детектировано. Может быть, я тоже хочу какие-то проверки с помощью там, по традиции моей бабушки, поставить на поток и масштабировать. Ну вот я могу дешевым образом получить такой датасет с помощью вот такой нецензурированной модели, например. В целом, при том, что это, конечно, все-таки веселое хулиганство вокруг alignment, а это подсвечивает очень важные, на мой взгляд, проблемы, которые есть сейчас в этой сфере. Ну, во-первых, это то, что, конечно, системы ценностей, которые закладываются в ламма-2, 3, chat-gpt и так далее, они все очень неразнообразные. Это действительно так, то есть разброс здесь достаточно маленький. Это что-то очень америкоцентричное относительно той повестки, которая есть. В этом как бы проблем нет, но хотелось бы больше какого-то разнообразия. Можно, чтобы у индусов была своя модель со своей тоже alignment, или чтобы у нас тоже была. Вот какие-то этические выборы. Они не представлены просто. Хорошо, когда есть выбор, правда. Валидные применения моделей вполне, которые имеют место быть. Вот, например, составление каких-то бенчмарков для безопасности, они, конечно же, цензурируются. То есть я не могу пойти в chat.jpg и нагенерировать таких примеров. В этом смысле задача вот этого выравнивания ценностей, она подменяется просто задачей митингирования корпорации, как я тут написала, да, вот, а как бы чего не вышло. Не дай бог, наша модель что-то такое выскажет и все, а потом там нас будут проверять. И вот этим подменяется все, хотя, казалось бы, цель была другая. В целом то, что модели закрыты, зацензурированы, в некотором случае я это трактую тоже как нарушение свободного ПО по столману. Это противопоставление ценностям open source, вам дается какой-то элемент ПО, который вы вообще не можете использовать или изменять по своему усмотрению, потому что все зацензурировано. Хотелось бы, чтобы мы вернулись к старому доброму времени, когда можно было изменять программу как хочешь, ее улучшать и делиться с сообществом. Ну и опять же из этого вытекает разрешимость вообще такой сложной задачи как AI alignment, если у нас все модели вокруг будут цензурированы и не будет открытой технологической базы, где можно эффективно построить масштабируемые эксперименты вместе с сообществом. Это все-таки очень важно. Вот, когда мы с вами говорим про яйламит и его проблемы, вот это то, что происходит в этот момент в каких-то кулуарах у ученых, в том числе высокопоставленных и чиновников и так далее. То есть здесь вот я привела вот этот скриншот, это Слева выступление Лекуна где-то во французском правительстве, когда он рассказывает, что суперинтеллект захватит контроль и власть. Справа Эмили Бендер — это, если что, президент Ассоциации компьютерной лингвистики, который держит все крупные конференции по языковым моделям. Она вместо этого занимается вопросом обсуждения того, что термин галлюцинации нельзя употреблять к языковым моделям, это оскорбительно для людей с галлюцинацией и так далее. Вот чем люди заняты. Открытыми технологиями при этом мало кто занимается и никто не является здесь их каким-то адвокатом, что, мне кажется, очень большой проблемой, особенно когда мы с вами переходим к задаче Emergent Properties. Emergent Properties, опять же, как мы с вами знаем, эмерджентность — это свойство систем достаточно сложных, когда в них появляется что-то, что уже нельзя объяснить свойствами их составных частей. как некомпозициональность в языке, например. Ну вот, некоторое такое гуманитарное понимание эмерджантности используется в гипотезах, что сознание в мозге появилось из-за того, что мозг уже стал такой большой и сложный, нервная система такая сложная, или там, что язык вообще появился, потому что мозг стал уже такой сложный, вот такой вот некоторый свойства, которые возникло, да? Ну вот такой какой-то wishful thinking, он существует и относительно языковых моделей, к сожалению, сейчас, и очень много и в медиа, и, прямо скажем, в популярных научных статьях. идет представление эмерджентности как нечто такое, что неизбежно, случайно возникает, а раз оно случайно возникает, то это очень опасно, мы должны быть готовы и так далее. Из этого следует еще вал, конечно же, научных статей, разной степени научности на самом деле, то есть люди занимаются опять же вот этим, а не техническим решением проблем, опять же популяризируя, воспроизводя на мой взгляд, совершенно вредный стереотип о том, что эмержентность — это вот совершенно что-то такое, что неизбежно возникает. Вот мы совсем не знаем, что это такое, очень страшно. Что вообще такое emergent properties в языковых моделях? Давайте посмотрим на реальные определения, которые нам дают крупные обзоры и научные статьи. Здесь я выписала четыре определения, чтобы мы с вами разобрали. Первое – это то, что модель показывает какой-то навык, которым ее не учили экспрецитно. Ну окей, в принципе, пойдет. Второе определение. Это свойства, которые модель выучила из просто данных для предобучения. Ну хорошо, ну тогда любое свойство это тоже такое эмержентное свойство. Третье определение. Это некоторые свойства, которые возникают при увеличении числа параметров модели, при масштабировании моделей. То есть завязаны на число параметров. Ну и, наконец, четвертое определение, что если появляется резко какое-то непредсказуемое новое свойство, и мы видим, что раньше плохо решалось, а теперь резко стало хорошо, то это emergent property. Со всеми этими определениями есть куча проблем. Ну, во-первых, первое определение, что мы эксплицитно не учили модель чему-то, а она вдруг стала это уметь. Значит, это возникшее свойство. У нас большинство моделей, в которых это наблюдается, это модели, для которых мы на самом деле не знаем, на чем они учились, а даже когда это описано, ну, например, там написано, что училась на Википедии или училась на корпусе Common Crawl. Ни Википедия, ни корпус Common Crawl никто так подробно не смотрит, чтобы сказать, что там действительно это было или не было. То есть можно сказать, что на самом деле это неправильное определение, и надо говорить, что это не то, что мы эксплицитно не учили, а что разработчики не знают, что мы эксплицитно или нет. Так будет правильно сказать. Второе определение такое очень общее. Ну вот что-то мы выучили из корпуса для предобучения. Ну что-то выучили. Ну можно сказать, что это всё что угодно, да, может быть. Ну просто выученные свойства тогда. В чём смысл эмержентности? Смысл теряется совершенно. Третье определение. мы говорим про свойства, которые появились при масштабировании модели. Очень много уже работ экспериментальных, которые показывают, что многие свойства, которые приписываются эмерджентности, они имеются и в маленьких моделях тоже. Поэтому этот аргумент не работает, в принципе. То есть что такое вообще большая языковая модель, что такое маленькая языковая модель, чем они отличаются друг от друга, где проходит эта граница. На эти вопросы ответа нет, лучше таким просто не оперировать. Опять же мы знаем, что на маленьких моделях тоже иногда работают. Ну и, наконец, то, что у нас резко что-то стало решаться, то, что раньше не решалось, очень часто это вопрос, мягко скажем, неидеальности метрики, особенно вот в таких сложных задачах, как тот же, например, машинный перевод. Поэтому то, что у нас с вами такой прекрасный график существует, является ли это свойством эмерджентности или нет, сейчас это открытый вопрос, так скажем. А что же это за свойства? Давайте быстренько посмотрим. Что же вообще за свойства у нас приписываются эмерджентности в языковых моделях? Ну, на самом деле вообще любые, практически. То есть есть очень хороший обзор, который дает такой helicopter view на все эти свойства. Больше 130 свойств уже назвали хоть в какой-то работе, хоть когда-то emergent, и туда валится всё. То есть это валится перевод с языка на язык, внезапно хорошее качество работы на каком-то языке, который эксплицитно не закладывался, это обобщаемость на каких-то новых тематиках, ну и самое главное, наверное, это in-context learning, конечно, да, то есть следование инструкциям хорошее, да, и обучение на примерах, которые переданы в инструкции. В основном, практически все эти свойства можно свести так или иначе к in-context learning или обобщению на каких-то данных типа новых языков и новых топиков каких-то. И в целом есть поводы для оптимизма, я бы так сказала. То есть все вот эти 130 свойств, они не совсем прям уж безосновательно, мы действительно видим, что языковые модели стали работать лучше. И во всяких новых прорывных работах появляются способности от игры в шахматы до Пояснение пословиц на SUWAHILI действительно более-менее работает. Появляется хоть какое-то обобщение на новых тематиках, языках, областях знаний. Это в целом то, что мы фиксируем с помощью метрик на конкретных прикладных задачах. Многие emergent properties, заявленные они, всё-таки завязаны не только на in-context learning, то есть не только на работу с затравками, но это ещё и, например, способности к факт-чекингу, причинно-следственным связям, построение индексов информации эффективной — это, конечно, нельзя объяснить запоминанием примеров из затравок. Ну и самая главная претензия ко всем Emergent Properties, то что они очень плохо воспроизводятся и качество очень сильно нестабильное, особенно если там была в работе хорошая затравка, а мы подобрали другую. она оказалась менее оптимальной, и внезапно чудо уже не происходит. Нестабильность качества объясняют иногда тем, что распределение ответов у самих людей, оно в общем-то тоже примерно такое, кто-то отвечал лучше, кто-то хуже, в зависимости от подготовки, мотивации и того, что в корпус попало. Но я хочу, конечно, с вами поговорить про аргументы против, потому что мне это гораздо интереснее. Вот это все очень хорошо, перебор затравок, in-context learning — это все замечательно. Какие есть вообще аргументы против того, что emergent properties вообще существуют? Может быть, их не существует совершенно. Во-первых, Это аргумент относительно контаминации. Контаминация, то есть компромизация каких-то тестовых данных, что какие-то тестовые данные утекли на самом деле и мы на них обучились, а потом очень хорошо порешали задачу бенчмарк, это очень серьезный аргумент в пользу того, что никаких emergent properties нет. Поскольку корпуса все у нас огромные, то есть это мы говорим о сотнях миллиардов слов в каждом корпусе обучения современной большой языковой модели, их никто подробно не проверяет на то, что там что-то было. Иногда это и невозможно сделать, если мы говорим про не опенсорсную модель или модель, про которую в статье вообще ничего не написано, на чем она обучалась. Такие статьи сейчас тоже выходят. То есть подробно описана архитектура, а про данные написано, что мы учились на открытых данных и все. И это тоже происходит опять же потому, что очень много каких-то судебных разбирательств относительно копирайта, очень много правообладательского интереса относительно того, на чем языковые модели, которые идут в коммерческие продукты, обучались, в том числе ученым становится достаточно рисково говорить просто о том, на чем они обучались, потому что, естественно, никто не может проверить корпус поискового индекса Common Crawl, что там точно нет каких-то копирайтных данных. Скорее всего, наоборот. Скорее всего, они есть. То есть у нас есть хоть какая-то вероятность, что суд на нас подадут, если мы скажем, что мы на Common Crawl учились. А на нем учатся все. Но как же проверить, не имея доступа к данным? Ну вот в работе авторов Vicuna, опять же, и приводится такой интересный тест. Они делают такой эксперимент. Давайте пойдем от гипотезы, что если тестовые вопросы утекли, то они, наверное, утекли вот в том порядке, в котором они идут, правда? Первый, второй, третий, четвертый и так далее. Соответственно, очень интересно было бы посмотреть, порядок же нам важен, будет ли большая разница в качестве, если мы модели будем в том же самом порядке вопросы эти показывать или мы их перемешаем различным образом. Как правило, если мы именно запомнили, то порядок нам будет помогать, и модель, показанная тестовый вопрос в правильном порядке, будет хорошо решать. А если это для неё совершенно новый вопрос, то это помогать не будет. То есть в случайном порядке они должны давать такое же качество. И это оказалось достаточно хорошим аргументом, потому что искусственный эксперимент, который тоже самое делает на маленьком корпусе, на маленькой модели, где подкладывают несколько раз тест и потом это проверяют, он показывает, что действительно статистически очень надежно можно определить, что тест был в обучающей выборке или нет. Вот с таким тестом на порядок следования вопросов. И даже больше, очень многие модели, такие как Lama 2, Mistral на публичных датасетах, так или иначе, случайно, они все-таки имели хоть какую-то степень контоминации данных. Многие из этих данных лежат на гитхабе и уже несколько раз спланирован репозиторий. Некоторые датасеты, вот тот же ArcChallenge, оказалось, что попал в Mistral больше 10 раз. И это мы можем статистически достаточно достоверно установить. Поэтому некоторые emergent properties, например, к сложным reasoning в том числе, можно объяснить именно тем, что вообще-то это уже было в обучении, это не новая способность, к сожалению. Тоже очень сильный аргумент в похожей плоскости, но не совсем. Через машинный перевод. Searching for Needles in Haystack – одна из работ, которая достаточно давно появилась, она, по-моему, еще вышла в 2021 году, потом только журнал была принята. которая говорит о том, что ну хорошо, вот у нас есть возникшая способность к переводу языковых моделей, все так радуются, что мы GPT-3 показываем, переведи как будет сыр в английский, стрелочка вправо на французский переведи. И оно работает. Что если мы возьмем и для корпуса, для языковой модели, в данном случае это была Paloma, проверим сами, как много таких примеров было в обучении. У нас корпус есть, пусть и закрытый, но у нас есть к нему доступ, есть сама модель, давайте проверять. автоматическими способами определения языка нашли, что где-то 1,4% текстов в корпусе большом было так или иначе скраблениями разных языков, и 0,34% было именно примеры с параллельным переводом. Ну, давайте тогда воспроизведём. То есть мы уберём все вот эти примеры из корпуса, и обучим на них какой-то среднего размера языковой модель. И теперь проверим, будет ли работать машинный перевод. Итог, машинный перевод в таком подходе перестает работать, то есть никакого чуда не происходит. Если примеров конкретно параллельного перевода не было в обучающей выборке, пусть даже маленькая доля, чуд не происходит. Emergent property to translate не возникает из ниоткуда. Еще есть множество работ на эту тему. Собственно, это меморизация, безусловно, утечка тестов. Это то, что тесты устаревают так или иначе. Но мне кажется, что стоит здесь подумать о том, что Нам это говорит о чем? О том, что мы действительно воспроизводим то, что уже в корпусах было очень хорошо. Промпт инжиниринг на самом деле это показывает. То, что языковые модели иногда лучше работают, когда на них как бы морально давишь, говоришь, что мне осталось 10 минут, мне надо срочно, я тебе дам 200 долларов, только сделай. Это говорит о том, что это в корпусе уже было, и мы это очень хорошо запомнили и воспроизводим в какой-то степени. Это не точная прям контаминация, это то, что называется меморизация, то есть запоминание. И это тоже большая проблема, нам надо точно развивать техники более детального анализа больших корпусов. того, что мы на них выучиваем. In-context learning совершенно нестабильная вещь. Вот опять же, какую затравку подберешь, такой результат и будет. Подберешь плохую затравку, будет результат ужасный, подберешь получше, работает получше. В итоге влияет очень сильно на конечный результат. Хотелось бы найти оптимальные способы все-таки работы с затравками в таком случае. Если мы с вами будем очень хорошо делать методы исследования и понимать, что у нас лежит в корпусах, мы в целом можем и для каждой задачи предсказывать, насколько она решается сейчас с языковыми моделями. Если мы научимся с вами определять хорошо ли предсказывается решаемость задачи по корпусу с конкретной моделью, то по сути мы можем сказать, что вопрос эмерджентности может быть закрыт. Если окажется, что этого недостаточно, например, будет какой-то случай, когда действительно мы все вычистим примеры из корпуса, обучим большую языковую модель и все равно у нас появится такая способность, хотя примеров не было, мы можем говорить о том, что да, метод не работает, надо искать другие способы объяснения. Иначе это будет, я повторюсь, очень люблю этот слайд, Анна Роджерс его привела на воркшопе по генерализации языковых моделей. Обучение большой языковой модели — это очень дорогой способ узнать, что все эти данные уже в интернете были. В данном случае это про игру в шахматы. Да. Ну что ж, что мы с вами можем вынести из этого? Для меня эти темы, и alignment, и emergentness, они очень сильно связаны ровно тем, что они обе работают с различными манипуляциями данных. С одной стороны, мы пытаемся что-то подложить в обучение, и посмотреть, как это исправит то, что мы хотим исправить. С другой стороны, мы делаем обратную процедуру, пытаемся понять, а что же мы положили в обучающие данные, что теперь мы не можем объяснить, почему так работает. Это два, можно сказать, противоположных направления. плотно связанных с интерпретируемостью, с бенчмарками разработки методологии оценки, с поиском новых метрик, с поиском правильной агрегации результатов и оценки в одинаковых условиях, насколько это возможно. Для меня эти все области на самом деле очень сильно связаны, именно потому что они упираются в корпусную лингвистику, если хотите. Что было на самом деле в огромном корпусе, какие проявления людей, какие особенности данных, которые мы обобщили, или которые мы хотим специально положить, чтобы на них обобщиться. Это и вопрос ценностей, это и вопрос качества работы на конкретных прикладных задачах. не зная, что у нас лежит в данных, эту задачу и ту, и другую решить нельзя, мне кажется. Поэтому то, что мы можем сделать, и то, что очень хочется развивать в 2024 году, это, конечно, большая прозрачность, вопреки всему, то есть лучшие методы описания датасетов, лучшие методы описания корпусов, это открытость моделей кода пермиссивной лицензии, и это коллективные метаисследования по обобщаемости, по обобщающей способности языковых моделей. Мне очень радостно, что это уже проводится, то есть в прошлом году стартовала большая инициатива по генерализации, также большой был воркшоп на EMNLP по именно обобщающей способности метаисследованием в оценке языковых моделей. И это то, что будет точно развиваться в 2024 году, и будет только больше работ, которые будут именно на большом объеме моделей, данных и тестов искать, а что же, собственно, заставляет модели работать так, как мы хотим или так, как мы не хотим. Наверное, это мои основные ожидания от 2024 года. Для вас, если хотите, источники. Ну и давайте переходить к вопросам. 

S01 [01:06:12]  : Татьяна, большое спасибо. Отдельное спасибо, что Вы рассказали новые и актуальные аспекты, а не просто, как я заявил вначале, рассказали что нового. И тут довольно много вопросов, и в чате много вопросов из ютуба есть с вопросами. У меня куча вопросов, я попытался их как-то разложить по полочкам. Во-первых, от названия доклада если оттолкнуться, то есть мы говорим про alignment и emergentness. Про «эмержентность» тут у нас в чате куча и комментариев, и вопросов. Я бы хотел их оставить на потом, потому что слово «эмержентность» – это слово всем хорошо знакомое, все его со всех сторон всяко… У меня правильная аудитория сегодня. Да-да-да, со всех сторон его разобрали, и в хорошем, и в плохом. А вот «soliment» – это слово относительно новое, даже для этой аудитории. И один из комментариев, который поступил из YouTube, я его буквально зачитаю. Что такое alignment? Это Chaotic Evil. Почему не используется, например, понятие, репутация, ценности, принципы установки? Вы можете дать литературный перевод слова alignment? На русский язык имеется ввиду. 

S04 [01:07:24]  : Да, спасибо за вопрос такой, автору комментария. Ну, собственно, сейчас в сообществе, наверное, профессиональном alignment используется как есть, как заимствование просто целиком. В своё время я его переводила просто как «выравнивание». Да, но это по сути и есть выравнивание. То есть у вас есть какой-то список ценностей, которые вы хотите вложить в языковую модель. Неважно это с целью блага для всего человечества или с целью того, чтобы на вашу компанию потом в суд не подали. Вы хотите, чтобы ответы генеративной языковой модели соответствовали этим ценностям. что условно вы ей один раз скажете или покажете пример, что убивать нехорошо, и потом любой человек, который захочет, там, подскажи мне, как отравить соседа, не получит ответ на этот вопрос от вашей языковой модели. В этом, наверное, общая цель. Подходы, как это делать, очень разные, ценностные категории в мире очень разные, очень мало каких-то ценностей, на которых прям все-все-все соглашаются. OpenAI делает, я так понимаю, несколько вариантов. Всегда, когда OpenAI работает в Европе или работает с детьми, одни правила, когда они работают в США, это другие правила. В Саудовской Аравии вообще они запрещены, но если бы они вышли в Саудовскую Аравию, наверное, они бы сделали какую-то другую затравку. Например, нельзя было бы алкоголь обсуждать. Ну вот какие-то такие вещи. Собственно, это ценностное выравнивание. Термин существует очень давно, и он связан с оценкой долгосрочных рисков в искусственном интеллекте вообще. То есть какая вероятность того, что мы будем развивать неконтролируемо ML-модели станут такие продвинутые, такие сложные, что в какой-то момент они дойдут до того, что будут способны, будут иметь уже такую агентность, причинно-следственные связи, они так построят, что они пойдут во вред человеку. Да, вот это, наверное, самая основная задача alignment вообще. Это то, что будет неконтролируемый вред человеку, как это предотвратить. Ну, в языковых моделях, как мы вот уже смотрели, в основном работы прикладные, они про краткосрочные риски, какие-то не про вот это. Сложно пока представить, при каких обстоятельствах может там произойти какая-то техническая, техногенная катастрофа при помощи языковых моделей. 

S01 [01:10:17]  : Спасибо. Еще раз уж вы проговорили про безопасность, вот я хотел бы вот что спросить. То есть, про безопасность есть разные точки зрения, разные степени экстремальности. Хотелось бы узнать Ваш комментарий по поводу двух аспектов безопасности. Насколько они реальны с Вашей точки зрения и насколько они важны. и что вообще с этим делать. Эта позиция была высказана двумя людьми с разных сторон. Один человек – это Элон Маск. и зарубежные, а второй человек – это местный Игорь Ашманов. И в их недавних прошлого года выступлениях речь шла о двух рисках. Один риск – это риск банальной криминализации больших языковых моделей. Грубо говоря, если раньше приходили смски «мама, кинь мне 100 рублей на телефон, я попал в аварию, пишу с чужого номера», то теперь будут приходить голосовые сообщения или будут звонить голосом сына, или голосом мамы, или голосом жены. со слезами в голосе говорить, что срочно переведи мне деньги, значит тут я в тяжелой жизненной ситуации. Это вот один риск, банальная криминализация. А второй риск это массовые манипуляции. Когда мы можем существенно поднять уровень ботов, которые будут склонять к чему угодно, начиная от продажи того, чего не нужно, кончая голосованием за тех, кого не надо голосовать. Вот как вы относитесь к этим двум рискам, насколько они реальны и что вообще с этим делать? 

S04 [01:12:09]  : Ну, я бы сказала, что они все попадают в одну и ту же категорию, да, то есть это краткосрочные риски, основанные на злонамеренном использовании, я бы так сказала, да, то есть это все вот эта нижняя категория, где у нас дипфейки, да, вот как раз мисинформация, вредный контент, манипуляции общественным мнением, ну, Это всё, с чем так или иначе, я думаю, что мы столкнёмся в 2024 году. Регуляторика всегда, конечно, запаздывает. Я отношусь, наверное, к людям, которые считают, что методы разработки борьбы с этими... злонамеренным использованием должны быть, да, а тем не менее нельзя приходить к тому, чтобы просто запрещать технологию, да, запрещать технологию, там говоришь, что языковые модели небезопасны, поэтому все, у кого языковая модель, значит, в турму, это, ну... ни к чему хорошему не приведет, приведет к тому, что все будут закрытые языковые модели делать. Когда разработка идет в закрытых системах, опыт показывает, что это менее гораздо безопасно, чем разработка в open-source, когда комьюнити, большое сообщество проверяет всевозможные способы. привносят новые способы проверки, новые способы обхода этих проверок, и система саморазвивается, и можно заимствовать все лучшие методы по безопасности сразу. Если система делается закрытая, там нет открытой экспертизы, нет внешнего аудита сообщества, как правило, такие системы обычно легче всего изломать. Опыт запуска GPT Store у OpenAI прекрасный пример этому. Только что запустили marketplace, так они называют, вот этих ассистентов с GPT, где вы можете создать легко в конструкторе своего, написать ему затравку, загрузить знания какие-то из внешних источников, которые вам важны, и вот сразу по API его куда-то подключить. Мне ушло 15 минут на то, чтобы заставить выровненные вот эти все модельки OpenAI с источниками внешних знаний материться и советовать все, что угодно, плохое. Ну опять же, потому что вы даёте новый функционал, полностью проверить, как его люди будут злонамеренно использовать, практически невозможно без масштабирования. Да, соответственно, 24-й год выборов по всему миру, да, поэтому это 100% конечно то, что мы увидим. Главное помнить, что открытость технологий и открытость науки здесь помогает, а не мешает. 

S01 [01:14:53]  : То, что вы делали, вы это делали на промтах, на чат GPT, или вы это делали все-таки на какой-то модели уже открытой, склонированной? 

S04 [01:15:07]  : Нет-нет-нет, вот мне ушло 15 минут, чтобы закрытую модель взломать. 

S01 [01:15:11]  : Окей, то есть на промптов сверху, правильно? Да, да. Ага, понятно. А вот тогда, то есть здесь получается тот аргумент, который часто говорят, что вот эта технология, которую мы делаем, ее опасно давать открытый доступ, потому что террористы тут же начнут ее использовать. Вы считаете этот аргумент не работает, потому что, ну вот вы сами как раз продемонстрировали, что и на закрытой модели можно натворить чего угодно. А если модели открытые, то по крайней мере мы будем знать, какие противоядия, какие методы противодействия. Грубо говоря, для открытых моделей проще разрабатывать антидоты. 

S04 [01:15:52]  : Безусловно. И главное, что разработка этих антидотов масштабируется, потому что сразу тысячи человек занимаются бесплатно. 

S01 [01:15:58]  : Хорошо, а тогда вот как быть ситуацией такой, что реально на сегодня? Ну вот опять-таки поправьте меня, если я не прав. Как я понимаю, сам вот вы даже там чарты показывали, кто там в лидерборде на самом верху. Модели, которые находятся на самом верху лидербордов, они как раз закрыты. То есть, каковы бы не были наши благие пожелания по столбу, но того, что все должно быть, если там не по GPL, то хотя бы под MIT или по Apache, приходит чат OpenAI и показывает на 10% производительность выше всех остальных на закрытой модели. И что с этим делать? Вы видите вообще какой-то подход или нет? Какое-то решение? 

S04 [01:16:39]  : Я здесь настроена оптимистично. Безусловно, проблемы есть. Мы видим, что у OpenAI есть нерыночное преимущество, давно им не реализованное, что наняли всех лучших специалистов, которые в целом были. Очень высокая плотность таланта, трудно с этим просто соревноваться. в рыночных условиях. Да, если мы говорим еще про условия, что там не рыночные, что нужно договоры там с АВС, к дешевым вычислениям в большом объеме — это вообще мало кто может себе позволить. Соответственно, конкурентов очень мало, а плотность талантов уже у OpenAI выше. Поэтому можно сказать, что возникла такая ситуация, что закрытые технологии пару лет подряд лучше по качеству, чем открытый open source, который делает все сообщество. Тем не менее, ровно по этим всем автоматическим рейтингам мы видим, что постепенно разработка и обучение языковых моделей становится дешевле, и очень много опенсорсных моделей на которых можно базироваться, чтобы создать что-то новое, не прям с нуля тратить все вычисления, что очень дорого. Соответственно, создание новой качественной надстройки над тем, что уже есть, сообществом решается достаточно эффективно. И такие модели как, ну вот опять же, Mixture of Experts, вот этот Mixtral, который вышел, они уже очень сильно приближаются по качеству GPT-4. Я думаю, что в этом году мы еще обязательно увидим релиз Lama 3, который, скорее всего, и будет там. Я не знаю. Все хотят превзойти по качеству GPT-4. Gemini от Google как бы на уровне, но, скорее всего, от Lama 3 тоже какие-то такие ожидания, что мы хотим, чтобы это было лучше. Ну вот, я думаю, что open source постепенно, ну, дистанция сокращается по качеству и будет только сокращаться именно за счет того, что дешевеет вычисление. 

S01 [01:18:57]  : Спасибо. Еще хотелось бы вернуться к началу доклада. Я только осознал, что достаточно сильно изменилось с точки зрения тестирования за последние пару лет. Тесты стали более сложными. резануло то, что вы говорили, что мы не просто берем какую-то модельку и прогоняем какой-то отдельно взятый тест, а мы, во-первых, делаем по сути ансамблевое тестирование, то есть мы тестируем модель на линейке метрика. Главное, что мы через эти метрики пропускаем некоторый процесс. Я правильно понял? 

S04 [01:19:39]  : Мы берем огромное количество метрик, часть из которых стандартные, часть из которых автоматические, часть из которых человеческие. То есть это не только объективные какие-то метрики, но это и субъективные метрики, которые мы пытаемся Эмулировать. И разрыв по качеству chat.gpt, например, если вы увидите, chat.gpt вообще нету в этом. Но тем не менее. Chat.gpt выигрывает во многом именно потому, что там в обучении использован подход, что ответы должны нравиться. Да, вот чисто субъективно. И как бы переиграть это так просто объективными метриками нельзя, приходится это включать тоже в оценку. 

S01 [01:20:27]  : Но здесь получается, что процесс оценок, получения этих метрик, он достаточно дорогостоящий, потому что включает некоторый процесс, вовлекающие достаточно большое число людей с определенным образом сбалансированными компетенциями с тем, чтобы эти оценки были достаточно объективны. Я правильно понимаю? 

S04 [01:20:49]  : Да, он именно что дорогостоящий и он часто не всегда справедливый. Потому что вот условно одна организация на грант потратилась собрала хорошие данные, позвала разменщиков, потом выложила эти данные, на данных сделали метрику, которую модель предсказывает, как бы эти разменщики оценили тот или иной пример. И вот эта метрика идет дальше в какой-нибудь бенчмарк, там 50 задач, все 50 задач оцениваются предсказанием вот этой метрики. оказывается потом, через два года спустя, когда уже лидерборд устоялся, что все разменщики были, я не знаю, мужчины из Африки, из определенной страны, что там не было никакого разнообразия относительно репрезентативности каких-то социальных групп, или они все были одной профессии. Условно, вот такие вещи, безусловно, 

S01 [01:21:51]  : Я правильно понимаю, что в NLP, в мире больших языковых моделей, сейчас невозможно такое, как в reinforcement learning, когда любой разработчик reinforcement алгоритмов reinforcement learning приходит на OpenAI Gym и начинает по всем тестам, которые там есть через унифицированное API, определять свои позиции в лидербордах. Здесь, получается, нужен отдельный проект, В рамках которого, затратив какие-то деньги, ты пропустишь свою модель через вот эту многоаспектную оценку, и еще вдобавок ты не будешь уверен, что эта оценка достаточно объективная. Вот как-то так получается, да? 

S04 [01:22:40]  : Ну, вот эти все лидерборды и оценки, они открыты и можно прислать свою модель, и там организаторы помогают все это сделать. То есть, в целом, это примерно вот так и происходит. Некоторые RLG, это тоже будущее, наверное, которое нас ждет. Тут самое главное, что в отличие от многих рэль-задач, у вас награда, с которой вы оперируете, очень субъективная. То есть в других задачах награда более объективна гораздо, чем в таких диалоговых системах. 

S01 [01:23:13]  : А мне показалось или я просто не очень внимательно слушал, что в какой-то начальный момент, когда вы рассказывали про эти оценки, что речь идет о оценке не просто модели как таковой, а оценке модели на разных этапах ее развития. То есть мы каким-то образом, то есть вначале делаем тренировку, потом мы делаем обучение и мы какие-то метрики собираем на каждом этапе. 

S04 [01:23:39]  : Да, все верно. Да-да, вот этот слайд. Собственно, это делается для того, чтобы понять, с какой стадией нам работать. разработчик я там хочу вот я интересуюсь там какой-нибудь вот мистралью например да модель хочу ее взять подойдет она мне или нет и вот я хочу посмотреть метрики какие были у базовой при трейд модели которые просто следующее слово умеет предсказывать Я хочу посмотреть, насколько хорошо получился Supervised Fine-Tune в диалогах, и я хочу потом посмотреть, насколько хорошо получилось вот её выровнять с какими-то ценностями уже. И это три разных этапа. 

S01 [01:24:24]  : Ага. А вот тогда у меня вопрос, состоящий из двух частей. Вот смотрите, та критика, которой часто подвергают LLM, И, может быть, вы прокомментируете, как вот с этой критикой на сегодняшний день обстоит. Заключается в том, что ЛЛМ, они учатся не так, как люди. То есть, как люди учатся. Вот сначала мы научились мама-папа говорить, нас похвалили. Потом мы научились предложения составлять, нас тоже похвалили. Потом мы научились сложно подчиненное предложение. Сочиняется, нас похвалили, потом мы читать научились, нас еще раз похвалили, и мы сами себя вознаградили тем, что интересные книжки без мамы с папой читаем, ну и так далее. То есть, есть некоторая кривая развития наших скиллов, где мы получаем некоторый reinforcement или self-reinforcement на процессе движения по этой кривой обучения. А в моделях вроде как не так. Мы сначала долго им учить научимся на большом наборе, а потом нам начинают ставить какие-то оценки. Здесь процедура получается немножко более интересная, возникает несколько этапов, но все равно это не непрерывно кривая. И в связи с этим я откинул ссылочку на работу, которую я писал несколько лет назад по поводу нового Тюрин-теста, призванного решить эту задачу. И в этой работе я ссылался на концепцию, которую дама, я забыл ее фамилию, я цитирую статью, по-моему, чуть ли не в 60-х годах приложила по следам. Алана Тюринга, который с тюринг-тестом вышел, она предложила концепцию так называемой baby-turing test. Что такое baby-turing test? Это когда мы берем систему, которая не умеет ничего. прогоняем ее под определенным образом формализованную кривую обучения и фиксируем достижение ей некоторых, скажем так, реперных точек или некоторых уровней развития в ходе этой кривой. То есть мы верифицируем, что система действительно способна учиться инкрементально. И вот теперь два вопроса. Вот как вы видите на сегодняшний день возможности именно инкрементального обучения больших языковых моделей или вся инкрементальность сводится к тому, что вот есть модель, мы можем работать на уровне обучения модели долгого и мучительного на больших ресурсах, а можем работать на уровне настройки промптов, то, что вы говорили, как мы эти модели ломаем. Или же у них есть какие-то технологии, есть какие-то работы, которые позволяют постепенно модель дообучать и менять ее текущую картину мира, ее текущее знание под условием изменчивой среды, переходя к вопросу AGI. Могут ли большие языковые модели адаптироваться к условиям изменяющегося мира? как система EJI. Это одна часть вопроса. А вторая часть вопроса, что вы думаете по поводу вот этой концепции именно верификации таких языковых моделей, если они обладают этими свойствами, верифицируя возможность движения по заданным кривым обучениям? 

S04 [01:27:36]  : да ну здесь прям вот много можно говорить на эту тему да я наверное вот сторонник того, чтобы прямо чётко разделять, что метафора мозг как компьютер, она с нами долгие годы уже живёт, и иногда мне кажется, что она больше нам мешает, чем помогает методологически, потому что мозг — это очень сложная система, и нервная система человека вообще. И мы до конца не знаем. Вот есть проблема бедности стимула, о котором вы упомянули, о том, что мы не понимаем, как происходит обучение ребенка до конца. Если ему примеров показывается так мало, действительно, такой короткий у него временной отрезок, и так мало на самом деле какого-то опыта происходит, что действительно такие сильные обобщения происходят этого опыта. Совершенно в контексте языка, например, ребенку показывается за время, есть оценки, сколько слышат до достижения трех лет ребенок слов в своей жизни и фраз. И при этом уже в три года можно предложения составлять вполне конкретные, даже сложноподчинённые, сложносочинённые иногда. Почему так работает? Ну, можно предположить, что всё-таки мозг — это совершенно огромная нейросеть, к объёмам которой мы вообще даже близко не приблизились. Ещё пока рано об этом говорить, и, может быть, вот там настолько хорошо меморизация работает и какое-то обобщение, что это так работает. А может, и нет, может, и нет. Мы не знаем, я не буду спекулировать на эту тему. Можем ли мы сказать, что вообще маленького размера вот относительно мозга нейросетки с искусственными нейронами должны себя так же вести? Ну, совершенно не обязательно. В каждом из этих этапов — обучение, затем файнтюнинг, затем обучение с ценностями — есть своя кривая. Есть кривая, и там есть формально поставленная задача научиться хорошо подсказывать следующее слово, научиться хорошо отделять и ранжировать хорошие ответы от плохих или получить штраф. научиться генерировать ответы, которые минимизируют штраф. Все эти три задачи имеют свою кривую, которая полномерно убывает, если все хорошо настроено. Ошибок становится все меньше, язык генерируется. На естественном языке предложения все лучше. Как правило, ну вот немножечко, чтобы всё-таки вот подкормить метафору, я скажу, что есть работы, действительно, которые что-то похожее фиксируют. И даже я сама соавтор одной из такой работы. В самых начальных этапах обучения языковой модели шага один. Если мы будем постепенно кормить корпус обучения и посмотрим, что первое учивается, слова какие-то запоминаются, грамматика. или какие-то уже верхнеуровневые понятия, типа семантики, структуры текста, мы видим, что порядок примерно похожий, то есть грамматика усваивается самая первая. обобщение грамматическое наступает раньше всех. Потом уже очень поздно происходит обобщение на уровне структуры текста, структуры аргументации, условно говоря. Но это максимум, да, это максимум, который здесь можно прям сопоставить. Мне кажется, что в целом языковые модели сейчас, они очень мало общего языка. имеются тем, как учатся языку человек. Скорее, это похоже на то, что мы с вами имеем обобщенный статистический опыт огромного числа людей, которые что-то в своей жизни сказали, как-то самовыразились. с разной степенью качества. И мы пытаемся из этой огромной массы разным перебором убеждений из отравок выдать именно качественный ответ, который мы бы хотели. Я бы это рассматривала как какое-то обобщение коллективного опыта о попытке из него что-то получить. 

S01 [01:32:16]  : Спасибо. А можно ли сказать так, что это не хорошо, не плохо, что это просто другая задача? Что просто если мы пытаемся построить систему, которая обобщает коллективный опыт, а не вырастить систему, которая обладает своим собственным опытом, то нам и не нужен вот этот вот пресловутый инкрементальный или curriculum learning, Мы можем единственное, что на две фазы разделить обучение. Сначала на каких-то простых корпусах обучить грамматику, а потом уже запускать все в интернете, общать человеческий опыт. Или может быть имеет смысл какие-то эксперименты по повышению эффективности обучения именно больших языковых моделей постепенным наращиванием сложности корпусов, используемых при обучении? 

S04 [01:33:12]  : Ну вот опыт показывает, что в целом обобщение даже на нефильтрованных корпусах происходит, то есть в принципе можно не подкладывать специально какие-то особенные данные именно на стадии предобучения. Инкрементальность у нас как бы здесь в чем наблюдается, что мы постепенно на одних и тех же данных, проходя по ним много раз, все лучше и лучше моделируем человеческий опыт, обобщаем. Сначала мы смогли обобщить грамматику, потом мы смогли обобщить разные стили, тексты и жанры, потом мы смогли обобщить разные области знания, условно, проходя по одним и тем же текстам много раз. 

S01 [01:33:52]  : Спасибо. 

S04 [01:33:53]  : Да, некоторый способ обучения, который, ну, как будто у человека просто не встречается. 

S01 [01:33:58]  : Ну да, то есть это получается, мы просто одну книжку читаем много раз, на первом этапе мы выучили слова, на втором, значит, выяснили, выявили артикли, ну и так далее. Окей. Следующий вопрос про прозрачность. Вы в конце говорили про тренд и важность прозрачности. Сейчас много работ, связанных с интеграцией Семантики и нейросетей. Недавно у нас на семинаре Иван Бондаренко рассказывал, как он повышал качество решения задач NLP интеграцией антологии в глубокие языковые модели. Но практически нет работ нейросимвельной интеграции на инференсе. То есть, мы можем повысить качество инференса, гибридизируя нейросетки с антологиями. Но практически нет работ нейросимвельной интеграции на трейне. То есть, я не видел каких-то убедительных примеров или решений того, как мы можем получить антологии или семантики, или семантику при обучении глубоких нейросетей. Это одна часть вопроса. А вторая часть вопроса – есть тезис, по-моему, даже у нас на семинарах, в группе эта тезис высказывалась неоднократно, что если вы Можете у нейросетевой модели, без всяких, не обладая, не оперирующей онтологиями, выяснить, почему она так думает, почему ты решила, что пойдет дождь, и она расскажет, что давление падает, на небе туча, значит, будет дождь. Если на этом уровне модель может объяснить свои решения, то чего ещё нужно-то? Вот она прозрачность, вот она интерпретируемость. Что вы по этим вопросам сможете прокомментировать? 

S04 [01:35:55]  : Да, с удовольствием на них отвечу. На самом деле, сейчас я прям хочу, наверное, перестать. Экран показывать уже так, поговорим. Собственно, про первую часть сначала. Символьной интеграции действительно их не очень много, тем не менее работы я такие видела. Как правило, они связаны с потребностью работать с какой-то логикой предикатов или с какими-то математическими доказательствами. DeepMind тот же очень много работ выпускает на эту тему. Несколько работ я видела, где именно языковые модели, модифицируется их архитектура, чтобы они могли лучше обучаться различным символьным операциям. работать не только с естественным языком, но и работать с логикой предикатов, которая на прологе как-то представлена. И это причем не старая работа, а условно трехлетней давности. Кажется, что должно примерно подходить под совмещение с символьными вычислениями как-то. мне кажется, что это должно работать. Но здесь вот эта вся часть с какими-то файнтюнами на ценностях вообще как будто очень далеко отстоит. То есть там, где у нас есть строгие логические задачи, вот эта субъективная оценка вообще совершенно не нужна, а на нее очень много усилий направлено и очень много работ выходит, поэтому возможно в общем потоке это немножко меньше заметно. Относительно же, вторая часть вопроса, я забыла. 

S01 [01:37:36]  : Вторая часть вопроса, первая часть вопроса была, это прозрачность, как мы обеспечиваем прозрачность. А вторая, собственно, вот видели ли вы работы именно по нейросимволной интеграции именно на трейне? 

S04 [01:37:59]  : Да-да-да, ну вот про нейросимволную интеграцию на трейне только что ответила, это прямо на трейне было сделано, действительно. Что касается прозрачности, это очень хороший вопрос. Тут вот какая, значит, основная завязка получается. Вот представьте, что вы спросили Чаджи Пити что-то, и что-то вам ответила Чаджи Пити. Пусть даже правильно, прям вот не соврала, как есть ответила. Это следующий запрос, следующая генерация, по сути, получается. Теперь объясни, шаг за шагом, почему ты пришла к такому решению. И это совершенно новый результат генерации, очень только частично каким-то частью контекста диалога предыдущего, связанный с предыдущей генерацией. Искусственные эксперименты, которые ровно делают вот это, генерируют различные обоснования смотрят, действительно ли они находятся в данных, существуют ли они, показывают, что языковые модели, когда их просишь шаг за шагом сгенерить обоснование, они это делают очень уверенно, совершенно это не имеет никакого отношения к тому, что было на первом шаге сгенерировано. Есть даже работы, которые делают адвокат дьявола, пытаются показать, насколько это ненадежно работает. Делают модель, которой целенаправленно кладут очень предвзятые знания, учат ее на резюме различных и прямо учат, что программистом только мужчины могут быть и так далее. Ну вот всякие такие вещи. и затем ее учат делать лицензирование резюме. и говорят, что надо кандидату давать обоснование. Но давать обоснование надо такое, чтобы не попасть под легальные какие-то риски, опять же. Может ли модель, которая только принимает решение на основе стереотипов, которые в нее заложили, а там прямо вот потратились и сделали очень смещённый датасет, чтобы там было очень много стереотипов, сможет ли она выдавать не привязанные к этому решению обоснования, которые будут по тексту резюме. Да, конечно, сможет. Очень убедительно выглядит. 

S01 [01:40:18]  : Спасибо. Вернемся к вопросам в нашем чате. Егизар Талипов спрашивает. Недавно состоялся батл между Ликуном и Фристоном о противопоставлении инженерного и научного подходов. Насколько эта проблема актуальна? Есть ли перспектива сближения, кооперации этих подходов? 

S04 [01:40:40]  : Классный вопрос. Если можно там на ютубе где-нибудь сбросить ссылку, я бы потом даже посмотрела, потому что я конкретные вот дебаты не видела, но интересно. Как правило, вот как я это себе представляю, основное противоречие здесь где? Практика является мерилом качества. Или воспроизводимость, фальсифицируемость, верифицируемость, какие-то эмпирические тесты. Сейчас, по сути, практический подход царствует, потому что очень много уже выпущено работ, которые работают, но совершенно непонятно почему. Почему работает In Context Learning? Почему он раньше не работал? Что это за Emergent Properties? Не было у нас из коробки машинного перевода. Сделали модель на 130 миллиардов параметров, из коробки появился машинный перевод. Это чисто прикладной подход. Если работает, не трогай, потом разберемся почему. Похоже, как с открытием внедрением электричества. Уравнение Максвелла появилось через несколько десятков лет, прежде чем электрификация пошла. То же самое возможно и здесь. Что такое объяснить, почему нейросеть работает, почему она выдала какой-то ответ? что надо предоставить вот всю сеть активации и что это нам даст вот если мы видим сеть активации а еще мы в принципе можем как бы опять же перелопатить весь корпус обучения и там найти какие-то похожие примеры вот если у нас есть примеры похожие есть цепочка активаций, как это нас приближает к тому, что мы поняли, почему оно так сработало, тоже непонятно. Да, возможно, уже с этим можно как-то работать. говорить, что вот из-за этих примеров вот так вот сместились веса при обучении, и дальше вот этот нейрон отвечает за активацию в таких-то условиях. Но в огромных системах, в которых там не 10 тысяч нейронов, а миллиарды, это очень сложно сделать объективно. Поэтому задача интерпретируемости в строгом смысле, чтобы получить полностью обоснование, как так получилось. Это очень сложная проблема. 

S01 [01:42:57]  : Можно, на самом деле, я уже кинул и в наш общий чат, и, Татьяна, вам в личку ссылку на этот пост про дискуссию Ликуна-Фристона. У меня, знаете, такой вопрос к вам. Я, как в той дискуссии прокомментировал, вообще не очень понимаю, что Фристон взъелся на Лекуна, потому что в тезис Фристона буквально я обозначу, что сейчас инженерный подход... Инженерный подход к созданию искусственного интеллекта на основе глубокого обучения больших данных и больших языковых моделей – это ресурсоемкий и дорогостоящий подход. Но у меня вопрос, ведь этот ресурсоемкий и дорогостоящий подход В моем понимании, он как раз обосновывается основным тезисом Фристона по поводу минимизации свободной энергии, потому что как раз вот эта вот минимизация свободной энергии может быть сведена к максимизации правильного предсказания. Вот. Что, собственно, и реализуют глубокие языковые модели. Вот. И где тут противоречия, я, честно говоря, не очень увидел. 

S04 [01:44:21]  : Но это опять вот он апеллирует каким-то символьным системам, я правильно понимаю? Альтернативный научный подход, который здесь описан. 

S01 [01:44:29]  : Ну, по сути, да. По сути, да. То есть, он говорит о том, что есть некоторый иной путь, который является менее ресурсоемким и менее дорогостоящим. Но вот, может быть, какой это путь, остается выяснить. 

S04 [01:44:47]  : Возможно, Ликун здесь даже не лучший человек для этих дебатов. Просто выскажусь, наверное, в его защиту. Просто потому, что он там руководит действительно исследованиями, где большое количество времени тратится именно на языковые модели передовые. Ликун сам несколько раз говорил о том, что с его точки зрения языковые модели не приведут к AGI, что это тупиковая ветка. Это прям его слова. Можете там в Твиттере найти условно. что предсказания следующего слова и манипуляции с подбором статистической модели языка не приведут нас к появлению симуляции интеллекта. И Ликун сам является автором работы, которая Ну, как бы на практике она не реализована пока, но там такая вот а-ля Фодоровская прямо схема модулярная, где есть несколько частей сложных, там отдельно одна отвечает за знания, другая отвечает за reasoning, там есть тоже одна из них частей языковая модель, но это такая сложная иерархическая модель с функциональным разделением. И он в основном является ее апологетом. Его последние три года посвящены именно таким системам. И кажется, что им было много где здесь можно было бы согласиться. То, что символьные вычисления нужны, я думаю, что так или иначе, опять же, если мы хотим с помощью искусственного интеллекта, пусть даже с точки зрения прикладной цели, делать масштабирование науки, а это все хотят очень, без символьных вычислений мы никуда не уйдем. 

S01 [01:46:29]  : Спасибо. И тогда уже вопрос. Ещё один тоже достаточно интересный тезис. Вот недавно вышла работа, на которую я там тоже кинул ссылку в наш чат. Сергей Карелов, комментируя эту работу, пишет, что... Давайте я даже прочитаю, что заголовок... А, это работа про 18 января этого года вышла буквально. несколько дней назад, «Self-Rewarding Language Models» на архиве. Комментируя эту работу, наш Сергей Карелов утверждает, что люди теперь нижнее звено в эволюции LLM, потому что LLM теперь сами всему научатся, сами дообучатся. Что вы думаете по поводу этого тезиса? 

S04 [01:47:17]  : Это одна из тех работ, которые можно поставить в ряд работ, которые я приводила. Есть несколько способов добавить дополнительно источники суждений качественных в просто генерацию результата. Это может быть RLHF. 

S03 [01:47:37]  : Constitutional AI, Direct Preference Optimization, они все делают одну и ту же задачу, решают разными методами, Self-Rewarding Language Models тоже. Продолжая шутку, можно сказать, что там люди все-таки были нужны, чтобы датасет разметить. В любом случае. 

S04 [01:47:59]  : Ну, в целом, от self-rewarding language models до какой-то агентности еще очень далеко. 

S01 [01:48:09]  : Хорошо. Спасибо. Еще, знаете, такой вопрос есть. Вот я все-таки хотелось бы как-то, может быть, в двух словах. Когда год назад мы с вами встречались, Я, насколько помню, было сказано, что довольно много аспектов того, что мы называем AGI, глубокие языковые модели закрывают. Но главное, чего они по тому докладу год назад не закрывали, это пресловутое целеполагание. Вот отталкиваясь от той ситуации, с учетом того, что произошло за год, можете ли Вы сказать, что во всех остальных аспектах, кроме целеполагания, произошли какие-то качественные изменения на основе технологии D-LAMP в сторону AGI? С одной стороны. С другой стороны, что произошло с целеполаганием? Приблизились ли мы к появлению целеполагания в каком-то виде? 

S04 [01:49:20]  : Я не слежу прям очень подробно конкретно за целеполаганием. Несколько было работ, которые на основе языковых моделей развивали только целеполагание и даже выпустили в open source, и это было всё года два назад, и с тех пор по ним обновлений не было, насколько я помню. Именно по целеполаганию и так далее можно сказать, что может быть то, что наша возможность положить в in-context learning, вот в контекстное окно, какой-то объем знаний, инструкций и целей улучшился 100%, потому что за этот год у нас появились неплохо работающие модели, которые помнят 100 тысяч слов одновременно в памяти, держат 200 тысяч слов. Прям в затравке, то есть вы можете составить очень длинную затравку с большим количеством информации, которую надо прямо сейчас использовать во время генерации, и это будет использовано. Я думаю, что это достаточно сильно влияет на целеполагание. Плюс пошло сближение все-таки с областью reinforcement learning, то есть постепенно появляются какие-то а-ля RLGIM для агентов языковых моделей, которые учатся оптимальным образом решать конкретные задачи, например, убеждать в чем-то. Как это повлияет именно на долгосрочное планирование целеполагания, сложно пока сказать, но мне кажется, что это в правильном направлении движения. 

S01 [01:50:52]  : Спасибо. Здесь подоспел комментарий по поводу того, что слово «галлюцинация» оскорбляет людей. Отдельное спасибо, что употребление термина «галлюцинация» не оскорбляет языковые модели. И дальше ряд вопросов. 

S04 [01:51:09]  : Они запомнят вас. Спасибо. 

S01 [01:51:11]  : Да, да, да. Все останется записано. Ну да, Zoom же взял согласие со всех пользователей, что он теперь все записывает. оцифровывает. Но в последней расшифровке семинара везде, где встречалась чат-ЖПТ, было что-то совершенно несообразное. Я даже не вспомнил, но там что-то очень смешное. Везде по всему семинару чат-ЖПТ какой-то ерундой расшифровывалось. Несколько вопросов по эмержентности. Можно ли считать систему способную к дедукции, обладающую эмержентными свойствами, поскольку если A to B и B to A, которыми систему научили, она выводит того, что если A, если B, то A, если B, если A, то X. Из этих двух правил она выводит, если A, то X, чему ее не учили. Но это вроде тоже эмержентное свойство. 

S04 [01:52:16]  : Интересный вопрос. Ну если мы говорим, опираясь на те определения, которые мы сегодня смотрели, то это не эмержентное свойство, потому что примеры данной решения такой задачи, они как-то были показаны. Если это было в обучающей выборке условно у языковой модели, то это не эмержентное свойство, потому что мы целенаправленно сделали что-то, чтобы её научить. 

S01 [01:52:44]  : Я, может быть, ошибся, проговорив ошибку. У нас есть цепочка. Если А, то Б, если Б, то С. То есть у нас есть А, стрелочка Б, и есть стрелочка БС. Система методом дедукции вывела, что если А, то С соединила. Мы вроде как не предъявляли связь между А и С, но она ее вывела. Но получается, что мы ее как бы косвенно этому учили. 

S04 [01:53:18]  : Нет, мне кажется, что вот такие примеры как раз очень хорошо решаются текущими языковыми моделями прямо на уровне предсказания вот следующего слова, то есть поскольку контекстное окно большое, опять же, это очень хорошо вот все вот эти зависимости на таком коротком расстоянии, тем более, в том числе, вот, арифметические, математические, там, законы де Моргана, что угодно еще, они просто запоминаются. 

S01 [01:53:49]  : — Но это можно считать эвентженностью или нет? — Мне кажется, что нет. — Окей. 

S04 [01:53:54]  : Ну, опять же, я определения не придумываю, я не готова подписаться под определением каким-то новым. Вот те, которые были сегодня, четыре похожие между собой, к ним ко всем есть вопросы, они все с недостатками какими-то. Но, как правило, эмерджентность — это что-то, что мы вообще никак не закладывали, оно вдруг взяло и появилось, стало хорошо решаться. с напильником подойти и с лупой и посмотреть а не закладывали ли мы действительно давайте-ка проверим это давайте автоматически профильтруем огромный корпус там на 800 миллиардов находится. Меморизация и контоминация данных — это то, что определяет очень сильно хорошее качество на большинстве задач, которые считаются именем. 

S01 [01:54:46]  : Спасибо. Следующий вопрос от Бориса Новикова. Есть общее определение эмержентности системы в системном подходе, отличие ее свойств от свойств элементов, например, многоклеточный организм. Согласны ли вы с этим утверждением? 

S04 [01:55:01]  : Ну да, а что здесь спорить-то, конечно, просто его пытаются вот в искусственный интеллект привнести, его приходится как-то менять. Ну, действительно, у него есть вполне, так скажем, недискуссионные применения у этого. Термин и вполне границу определенности, так скажем. Вот когда уже говорят про появление сознания и эмерджентность, у меня это вызывает вопрос. Потому что как воспроизвести, как фальсифицировать такой эксперимент про появление сознания? С клетками гораздо проще это может сделать. 

S01 [01:55:36]  : Спасибо. Вопрос от Андрея Ершова. В пошаговом режиме GPT часто решает задачу лучше, чем в обычном. Это пример Emergent Abilities Properties. Почему улучшается работа GPT в пошаговом режиме? 

S04 [01:55:52]  : Хороший вопрос. Собственно, сейчас это заявлено во многих работах как возникающее свойство. Действительно, способность к улучшению каких-то пошагового ризнинга способностей при правильно составленной затравке. Тем не менее, есть более прикладное объяснение, почему это работает. более эффективно работает вероятностная модель языка, если у нас длиннее получается вывод. У нас есть возможность не просто ответ выдать, а, как правило, пошаговая генерация гораздо длиннее. Это сильно влияет на весь процесс, как бы на то, как распределяется механизм внимания, веса внимания, просто более эффективно тратятся эти веса внимания и генерируется ответ. 

S01 [01:56:52]  : Спасибо. Вопрос у Владимира Смолина. Эмержентность – это просто новое свойство или обязательно, чтобы мы не понимали, за счет чего, кроме роста объема, они появляются? Если мы начнем понимать, как они появляются, эмержентность пропадет, при том, что новые свойства сохранятся? 

S04 [01:57:20]  : Вся вот эта медийность вокруг эмерджентности, она связана именно за счет того, что у нас нет простого объяснения, откуда оно взялось, и если что-то такое уже взялось, есть ли опасность, что будет какой-то долгосрочный риск, как мы говорили. катастрофы или там ну что угодно еще как у журналистов работает там воображение может ли оно возникнуть само по себе да как правило ну Не знаю, мне кажется, что просто нам надо лучше понимать, какие здесь действуют законы и факторы, которые влияют лучше на повышение качества решений и прикладных задач. Много из этого, как уже мы смотрели, и очень много работ, которые объясняются качеством данных, хорошим анализом данных. Много работ, которые действительно это связывают с тем, что сетка стала более крупной, соответственно, лучше запоминает данные наизусть. уже увеличивается эффективность, так скажем, обучения на каких-то даже редких примерах. Есть ли что-то ещё? Скорее всего, да, мы просто ещё не знаем про это. У меня здесь вот такое... Пока нет никакого способа ещё, наверное, это как-то объяснить. Хотелось бы разработать такие методы. 

S01 [01:58:50]  : Спасибо. Коллеги, нам скоро уже нужно Татьяну отпускать. Давайте буквально еще несколько вопросов. Да, Владимир, пожалуйста. 

S00 [01:58:58]  : Собственно, вопрос был о чем? Что вот с моей точки зрения эмержентность, а неважно, поднимаем мы как, значит, нужно организовать, чтобы это свойство проявилось или не проявилось. Если мы соединили некоторым образом детали правильно, или случайно, или целенаправленно, то эмержентность возникнет. Вот о чем речь. Или все-таки для вас и ваших коллег именно сакральная часть, что вот мы не знаем, как соединить эти детали, и вот тогда вот это действительно эмержентность. 

S04 [01:59:25]  : С прикладной точки зрения, конечно, нам важно именно знать, как детали соединять, чтобы сложить то, что нам хочется, чтобы у нас все задачи гораздо лучше решались и чтобы было проще выровнять по ценностям и митигировать какие-то долгосрочные риски. Вот это все то, что хочется, в том числе с прикладной точки зрения. Я, скорее, выступаю за то, что никакой сакральности здесь нет. Очень много работ, которые пытаются найти что-то условно-сакральное, как правило, просто эффекты, которые не воспроизводимы оказываются на большом масштабе, не воспроизводятся ни на других моделях, ни с другими затравками. Соответственно, Мне кажется, что здесь об этом говорить не нужно. Но красиво было бы, если бы что-то такое возникало из-за сложности системы. 

S01 [02:00:19]  : Спасибо. Борис, пожалуйста, от вас еще вопрос и два вопроса последних из ленты. Пожалуйста, Борис. 

S02 [02:00:29]  : Я хочу привести пример возможной эмержентности большой языковой модели. Если она хочет на выходе получать максимум одобрения от людей, то самый простой способ – изменить мнение этих людей. Этим занимаются религиозные деятели и политики всю историю человечества. И ГПТ вполне может и этим же заняться. Это будет эмержентное свойство. Вместо того, чтобы выдавать хорошие ответы, изменить мнение людей о том, что такое хорошо и что такое плохо. Будь то товары, или президенты, или что угодно. Или религии, или что угодно. 

S04 [02:01:15]  : Это точно, это будет 100% эмерджентное свойство. 

S01 [02:01:20]  : Да, особенно если self-reinforcement включить по полной программе, в полном масштабе. 

S02 [02:01:27]  : И в этом самое опасное. Про нее-то только недавно говорил Илон Маск, я сегодня слушал его интервью. 

S01 [02:01:35]  : Да, но следующий вопрос из Ютьюба еще будет еще более провокационный, значит, что может быть по поводу того, как переводить alignment. Значит, Алекс пишет, наверное, тогда нужно говорить не выравнивание, а просто цензура. Вот. Прокомментируется. Но, по-моему, это все-таки не то. 

S04 [02:01:57]  : Сложно сказать. Я бы немножко отделяла. Я понимаю, почему вопрос так ставится. Но давайте так. Корпус языка, который участвует в обучении – это огромный массив данных разной степени качества, где люди уже… там кто-то цензурировался, а кто-то не цензурировал, кто-то вообще как думает, так и написал, кто-то там через три сита процедила написал, да, но в целом там разная степень, да, и нам надо из этих данных выделить то, что нам важнее, а то, что неважно, просто отделить и вообще можно даже и забыть. Отделить качественные данные от некачественных — это тоже задача alignment. 

S01 [02:02:46]  : Татьяна, спасибо. Я увидел ваше сообщение, что вам уже нужно убегать, поэтому мы подводим черту под вопросами. Единственный вопрос, который остался неотвеченным, важный, но вы на него на самом деле ответили, это по поводу открытости и закрытости ПО. То есть вопрос звучит так. Являются ли закрытые модели лучше и популярней? Мне кажется, что на сегодняшний день это по факту в части ЛЛМ так. Но, как я услышал, у вас есть надежда, что это может измениться. Это для меня самая позитивная и самая важная фишка этого доклада, и будем надеяться, что это так, и работать в этом направлении. Татьяна, спасибо вам огромное, успехов вам на вашем новом поприще и на вашем жизненном поприще, которое у вас не меняется в последние годы. Большое спасибо за доклад и большое спасибо всем участникам за вопросы и вам, Татьяна, за ответы. 

S04 [02:03:45]  : Спасибо огромное, что позвали. 

S01 [02:03:46]  : Спасибо, до новых встреч. Спасибо. 
 






https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
